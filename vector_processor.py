import numpy as np
import streamlit as st
from sentence_transformers import SentenceTransformer
import os
from typing import List, Dict, Any
import torch
import json
import subprocess


class VectorProcessor:
    def __init__(self, user_model_dir="./hf_embedding_cache"):
        """
        åˆå§‹åŒ–å‘é‡å¤„ç†å™¨ï¼Œæ”¯æŒç”¨æˆ·åŠ¨æ€æ·»åŠ æœ¬åœ°åµŒå…¥æ¨¡å‹ï¼Œå¹¶è‡ªåŠ¨æ‰«ææœ¬åœ°æ¨¡å‹ç›®å½•ã€‚
        """
        self.model = None
        self.model_name = None
        self.dimension = None
        self.user_model_dir = user_model_dir
        self.available_models = self.scan_local_models()
        self.model_select_idx = 0  # é»˜è®¤é€‰ä¸­ç¬¬ä¸€ä¸ª

    def scan_local_models(self):
        """
        æ‰«ææœ¬åœ°æ¨¡å‹ç›®å½•ï¼Œè¿”å›æ‰€æœ‰æ¨¡å‹åç§°åˆ—è¡¨
        """
        models = []
        if os.path.isdir(self.user_model_dir):
            for entry in os.listdir(self.user_model_dir):
                full_path = os.path.join(self.user_model_dir, entry)
                if os.path.isdir(full_path):
                    models.append(entry)
        return models

    def _ping_huggingface(self):
        """
        ä½¿ç”¨ curl å‘½ä»¤åˆ¤æ–­æ˜¯å¦èƒ½è¿æ¥ huggingface.co
        """
        try:
            # -I åªè¯·æ±‚å¤´ï¼Œ--connect-timeout 3 æœ€å¤šç­‰3ç§’
            curl_cmd = ["curl", "-I", "--connect-timeout", "3", "https://huggingface.co"]
            result = subprocess.run(curl_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            # åªè¦ exit code ä¸º 0 ä¸”è¿”å›æœ‰ HTTP çŠ¶æ€è¡Œå³è®¤ä¸ºå¯è®¿é—®
            if result.returncode == 0 and b"HTTP" in result.stdout:
                return True
            else:
                return False
        except Exception:
            return False


    def _download_model(self, model_name):
        """
        ä½¿ç”¨ huggingface-cli ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ° embedding cache ç›®å½•ã€‚
        æ”¯æŒç”¨æˆ·ç›´æ¥è¾“å…¥å®Œæ•´æ¨¡å‹åç§°ï¼ˆå¦‚ sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2ã€Qwen/Qwen3-Embedding-4Bã€BAAI/bge-m3 ç­‰ï¼‰ã€‚
        """
        st.info(f"ğŸŒ æ­£åœ¨å°è¯•ä¸‹è½½æ¨¡å‹: {model_name} åˆ° {self.user_model_dir}")
        os.makedirs(self.user_model_dir, exist_ok=True)
        try:
            # ç”¨æˆ·è¾“å…¥çš„ model_name åº”ä¸ºå®Œæ•´çš„ huggingface ä»“åº“åï¼ˆå¦‚ Qwen/Qwen3-Embedding-4Bï¼‰
            cmd = [
                "huggingface-cli", "download",
                model_name,
                "--cache-dir", self.user_model_dir
            ]
            st.info(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                st.success("âœ… æ¨¡å‹å·²æˆåŠŸä¸‹è½½åˆ°æœ¬åœ°ç¼“å­˜ç›®å½•")
                return True
            else:
                st.error(f"âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {result.stderr}")
                return False
        except Exception as e:
            st.error(f"âŒ æ¨¡å‹ä¸‹è½½æ¨¡å‹å¼‚å¸¸: {e}")
            return False


    def select_and_add_model_ui(self):
        """
        å±•ç¤ºæ·»åŠ åµŒå…¥æ¨¡å‹è¾“å…¥æ¡†å’Œæ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†ï¼Œè¿”å›ç”¨æˆ·é€‰æ‹©çš„æ¨¡å‹å
        """
        st.markdown("### 1. æ·»åŠ æˆ–é€‰æ‹©æœ¬åœ°åµŒå…¥æ¨¡å‹")
        st.info(f"è¯·å°† HuggingFace SentenceTransformer æ ¼å¼çš„æ¨¡å‹æ–‡ä»¶å¤¹æ”¾ç½®äº `{self.user_model_dir}/æ¨¡å‹å`ï¼Œæˆ–åœ¨ä¸‹æ–¹è¾“å…¥æ¨¡å‹åç§°è‡ªåŠ¨ä¸‹è½½ã€‚")

        # è¾“å…¥æ¡†ï¼šå…è®¸ç”¨æˆ·æ·»åŠ æ–°æ¨¡å‹å
        new_model_name = st.text_input("æ·»åŠ æˆ–ä¸‹è½½æ–°æ¨¡å‹ï¼ˆè¾“å…¥æ¨¡å‹åç§°ï¼Œå¦‚ paraphrase-multilingual-MiniLM-L12-v2 ï¼‰", value="")

        # ä¸‹è½½æ–°æ¨¡å‹é€»è¾‘
        if new_model_name:
            if new_model_name not in self.available_models:
                # ä¸‹è½½å‰å…ˆping huggingface
                if self._ping_huggingface():
                    st.info("HuggingFace å®˜ç½‘å¯è¾¾ï¼Œæ­£åœ¨è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ã€‚")
                    download_success = self._download_model(new_model_name)
                    if download_success:
                        # åˆ·æ–°æ¨¡å‹åˆ—è¡¨ï¼Œè‡ªåŠ¨é€‰ä¸­æ–°æ¨¡å‹
                        self.available_models = self.scan_local_models()
                        if new_model_name in self.available_models:
                            self.model_select_idx = self.available_models.index(new_model_name)
                            st.success(f"å·²æ·»åŠ æ–°æ¨¡å‹: {new_model_name}ï¼Œæ— éœ€å†æ¬¡ä¸‹è½½ã€‚")
                else:
                    st.error("âŒ æ— æ³•è®¿é—® huggingface.coï¼Œæ— æ³•è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ã€‚è¯·æ‰‹åŠ¨å°†æ¨¡å‹æ–‡ä»¶å¤¹æ”¾ç½®åˆ°æœ¬åœ°ï¼Œå¹¶åœ¨ä¸‹æ‹‰æ¡†ä¸­é€‰æ‹©ã€‚")
                    st.info(f"ä½ å¯ä»¥ç”¨å‘½ä»¤/huggingface-cliç¦»çº¿ä¸‹è½½æ¨¡å‹ï¼Œç„¶åå°†å…¶è§£å‹åˆ° {self.user_model_dir}/{new_model_name}")
        # ä¸‹æ‹‰æ¡†ï¼šé€‰æ‹©å¯ç”¨æ¨¡å‹
        if self.available_models:
            selected = st.selectbox(
                "é€‰æ‹©æœ¬åœ°åµŒå…¥æ¨¡å‹",
                options=self.available_models,
                index=self.model_select_idx,
                key="embedding_model_select"
            )
            self.model_name = selected
            st.info(f"å½“å‰é€‰ä¸­æ¨¡å‹: {selected}ï¼Œè·¯å¾„: {os.path.join(self.user_model_dir, selected)}")
        else:
            st.warning(f"æœ¬åœ°æ¨¡å‹åº“ `{self.user_model_dir}` ä¸ºç©ºï¼Œè¯·æ·»åŠ æ¨¡å‹æˆ–å…ˆä¸‹è½½ï¼")
            self.model_name = None

    def load_model(self) -> bool:
        """
        åŠ è½½å®é™… snapshot å­ç›®å½•ä¸‹çš„æ¨¡å‹
        """
        if not self.model_name:
            st.error("âŒ æœªé€‰ä¸­æ¨¡å‹ï¼Œè¯·å…ˆæ·»åŠ /é€‰æ‹©æœ¬åœ°åµŒå…¥æ¨¡å‹ã€‚")
            return False

        # 1. è‡ªåŠ¨æ‰¾åˆ° snapshot ç›®å½•
        base_dir = os.path.join(self.user_model_dir, self.model_name, "snapshots")
        if not os.path.isdir(base_dir):
            st.error(f"âŒ æœ¬åœ°æ¨¡å‹ç›®å½•ç¼ºå¤±çš„ snapshots ç›®å½•: {base_dir}")
            return False
        snapshot_dirs = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]
        if not snapshot_dirs:
            st.error(f"âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½• snapshot å­ç›®å½•ï¼Œè¯·æ£€æŸ¥æ¨¡å‹ä¸‹è½½æ˜¯å¦å®Œæ•´ï¼")
            return False
        model_path = os.path.join(base_dir, snapshot_dirs[0])
        st.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹ç›®å½•: {model_path}")

        device = "cuda" if torch.cuda.is_available() else "cpu"
        try:
            self.model = SentenceTransformer(model_path, device=device)
            test_vector = self.model.encode(["æµ‹è¯•æ–‡æœ¬"])
            if len(test_vector.shape) == 2:
                self.dimension = test_vector.shape[1]
                st.success(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå‘é‡ç»´åº¦: {self.dimension}")
                return True
            else:
                st.error(f"âŒ æ¨¡å‹è¾“å‡ºç»´åº¦å¼‚å¸¸: {test_vector.shape}")
                return False
        except Exception as e:
            st.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False

    def preprocess_text(self, text: str) -> str:
        """
        æ–‡æœ¬é¢„å¤„ç†
        """
        if not text:
            return ""
        
        # åŸºæœ¬æ¸…ç†
        text = text.strip()
        # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤šé¢„å¤„ç†æ­¥éª¤
        return text
    
    def parse_json_file(self, file_content: str) -> List[Dict[str, Any]]:
        """
        è§£æJSONæ–‡ä»¶å†…å®¹ï¼Œæ”¯æŒå¤šç§æ ¼å¼
        """
        json_data = []
        
        try:
            # å°è¯•ç›´æ¥è§£æä¸ºJSON
            data = json.loads(file_content)
            if isinstance(data, list):
                json_data = data
            elif isinstance(data, dict):
                json_data = [data]
            else:
                st.error("JSONæ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®ï¼Œåº”ä¸ºå¯¹è±¡æˆ–å¯¹è±¡æ•°ç»„")
                return []
        except json.JSONDecodeError as e:
            # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•æŒ‰è¡Œè§£æï¼ˆJSONLæ ¼å¼ï¼‰
            st.info("å°è¯•æŒ‰JSONLæ ¼å¼è§£æ...")
            lines = file_content.strip().split('\n')
            
            for i, line in enumerate(lines):
                line = line.strip()
                if not line:
                    continue
                    
                try:
                    item = json.loads(line)
                    json_data.append(item)
                except json.JSONDecodeError as line_error:
                    st.warning(f"ç¬¬{i+1}è¡ŒJSONè§£æå¤±è´¥: {line_error}")
                    st.warning(f"é—®é¢˜è¡Œå†…å®¹: {line[:100]}...")
                    continue
            
            if not json_data:
                st.error(f"æ— æ³•è§£æJSONæ–‡ä»¶ã€‚åŸå§‹é”™è¯¯: {e}")
                st.error("è¯·ç¡®ä¿æ–‡ä»¶æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œä¾‹å¦‚ï¼š")
                st.code('[{"text":"æ–‡æœ¬å†…å®¹1"}, {"text":"æ–‡æœ¬å†…å®¹2"}]')
                st.error("æˆ–è€…JSONLæ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼‰ï¼š")
                st.code('{"text":"æ–‡æœ¬å†…å®¹1"}\n{"text":"æ–‡æœ¬å†…å®¹2"}')
                return []
        
        return json_data
    
    def process_json_data(self, json_data: List[Dict[str, Any]]) -> tuple:
        """
        å¤„ç†JSONæ ¼å¼çš„æ•°æ®
        è¿”å›æ–‡æœ¬åˆ—è¡¨å’Œå¯¹åº”çš„å‘é‡
        """
        texts = []
        metadata = []
        
        for i, item in enumerate(json_data):
            if isinstance(item, dict):
                for key, value in item.items():
                    if isinstance(value, str) and value.strip():
                        texts.append(value)
                        metadata.append({
                            'id': i,
                            'key': key,
                            'original_data': item
                        })
            elif isinstance(item, str) and item.strip():
                # å¦‚æœç›´æ¥æ˜¯å­—ç¬¦ä¸²
                texts.append(item)
                metadata.append({
                    'id': i,
                    'key': 'text',
                    'original_data': {'text': item}
                })
        
        if not texts:
            return [], [], []
        
        # ç¼–ç æ–‡æœ¬
        vectors = self.encode_texts(texts)
        
        return texts, vectors, metadata
    
    def encode_texts(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """
        å°†æ–‡æœ¬åˆ—è¡¨ç¼–ç ä¸ºå‘é‡
        """
        if not self.model:
            st.error("âŒ æ¨¡å‹æœªåŠ è½½")
            return np.array([])
        
        try:
            st.info(f"ğŸ”„ æ­£åœ¨å¤„ç† {len(texts)} æ¡æ–‡æœ¬...")
            
            # åˆ›å»ºè¿›åº¦æ¡
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            all_vectors = []
            total_batches = (len(texts) + batch_size - 1) // batch_size
            
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                
                # é¢„å¤„ç†æ–‡æœ¬
                processed_texts = [self.preprocess_text(text) for text in batch_texts]
                
                # ç¼–ç å½“å‰æ‰¹æ¬¡
                batch_vectors = self.model.encode(
                    processed_texts,
                    convert_to_numpy=True,
                    show_progress_bar=False,
                    batch_size=min(batch_size, len(batch_texts))
                )
                
                all_vectors.append(batch_vectors)
                
                # æ›´æ–°è¿›åº¦
                current_batch = (i // batch_size) + 1
                progress = current_batch / total_batches
                progress_bar.progress(progress)
                status_text.text(f"âœ… å·²å¤„ç† {min(i + batch_size, len(texts))}/{len(texts)} æ¡æ–‡æœ¬")
            
            # åˆå¹¶æ‰€æœ‰å‘é‡
            vectors = np.vstack(all_vectors)
            
            progress_bar.progress(1.0)
            status_text.text(f"ğŸ‰ æ–‡æœ¬å‘é‡åŒ–å®Œæˆï¼ç”Ÿæˆ {vectors.shape[0]} ä¸ª {vectors.shape[1]} ç»´å‘é‡")
            
            return vectors
            
        except Exception as e:
            st.error(f"âŒ æ–‡æœ¬ç¼–ç å¤±è´¥: {e}")
            return np.array([])
        finally:
            # æ¸…ç†è¿›åº¦æ˜¾ç¤º
            if 'progress_bar' in locals():
                progress_bar.empty()
            if 'status_text' in locals():
                status_text.empty()
    
    def encode_single_text(self, text: str) -> np.ndarray:
        """
        ç¼–ç å•ä¸ªæ–‡æœ¬ä¸ºå‘é‡
        """
        if not self.model:
            st.error("âŒ æ¨¡å‹æœªåŠ è½½")
            return np.array([])
        
        try:
            processed_text = self.preprocess_text(text)
            vector = self.model.encode([processed_text], convert_to_numpy=True)
            return vector[0]  # è¿”å›å•ä¸ªå‘é‡
        except Exception as e:
            st.error(f"âŒ å•æ–‡æœ¬ç¼–ç å¤±è´¥: {e}")
            return np.array([])
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        """
        if not self.model:
            return {}
        
        return {
            'model_name': self.model_name,
            'dimension': self.dimension,
            'device': str(self.model.device),
            'max_seq_length': getattr(self.model, 'max_seq_length', 'Unknown'),
            'cache_dir': self.user_model_dir,
            'embedding_dimension': self.model.get_sentence_embedding_dimension()
        }
    
    def encode(self, texts):
        """
        å¯¹è¾“å…¥æ–‡æœ¬åˆ—è¡¨è¿›è¡Œå‘é‡åŒ–ç¼–ç 
        """
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆåŠ è½½åµŒå…¥æ¨¡å‹ã€‚")
        return self.model.encode(texts)

    def calculate_similarity(self, vector1: np.ndarray, vector2: np.ndarray) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
        """
        try:
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            dot_product = np.dot(vector1, vector2)
            norm1 = np.linalg.norm(vector1)
            norm2 = np.linalg.norm(vector2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            similarity = dot_product / (norm1 * norm2)
            return float(similarity)
            
        except Exception as e:
            st.error(f"âŒ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    

