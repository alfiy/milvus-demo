import numpy as np
import pandas as pd
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score
import plotly.express as px
import plotly.graph_objects as go
from umap import UMAP
import streamlit as st
from typing import List, Dict, Any, Tuple
import warnings

# æŠ‘åˆ¶ UMAP å’Œ Numba è­¦å‘Š
warnings.filterwarnings('ignore', message='.*n_jobs.*overridden.*')
warnings.filterwarnings('ignore', message='.*TBB threading layer.*')

class ClusteringAnalyzer:
    def __init__(self):
        """
        åˆå§‹åŒ–èšç±»åˆ†æžå™¨
        """
        self.vectors = None
        self.texts = None
        self.metadata = None
        self.cluster_labels = None
        self.reduced_vectors = None
    
    def load_data(self, vectors: np.ndarray, texts: List[str], metadata: List[Dict]):
        """
        åŠ è½½æ•°æ®
        """
        self.vectors = vectors
        self.texts = texts
        self.metadata = metadata
    
    def perform_kmeans_clustering(self, n_clusters: int = 8, random_state: int = 42) -> np.ndarray:
        """
        æ‰§è¡ŒK-meansèšç±»
        """
        if self.vectors is None:
            st.error("è¯·å…ˆåŠ è½½æ•°æ®")
            return np.array([])
        
        try:
            kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)
            self.cluster_labels = kmeans.fit_predict(self.vectors)
            
            # è®¡ç®—è½®å»“ç³»æ•°
            if len(set(self.cluster_labels)) > 1:
                silhouette_avg = silhouette_score(self.vectors, self.cluster_labels)
                st.info(f"âœ… K-meansèšç±»å®Œæˆï¼Œè½®å»“ç³»æ•°: {silhouette_avg:.3f}")
            
            return self.cluster_labels
            
        except Exception as e:
            st.error(f"âŒ K-meansèšç±»å¤±è´¥: {e}")
            return np.array([])
    
    def perform_dbscan_clustering(self, eps: float = 0.3, min_samples: int = 5) -> np.ndarray:
        """
        æ‰§è¡ŒDBSCANèšç±»
        
        Args:
            eps: é‚»åŸŸåŠå¾„
                - cosineè·ç¦»ï¼šæŽ¨èèŒƒå›´ 0.1-0.5ï¼ˆå€¼è¶Šå°è¶Šä¸¥æ ¼ï¼‰
                - euclideanè·ç¦»ï¼šéœ€è¦æ ¹æ®æ•°æ®å°ºåº¦è°ƒæ•´
            min_samples: æ ¸å¿ƒç‚¹çš„æœ€å°é‚»å±…æ•°ï¼ŒæŽ¨è 5-10
        """
        if self.vectors is None:
            st.error("âŒ è¯·å…ˆåŠ è½½æ•°æ®")
            return np.array([])
        
        try:
            #  å…³é”®ä¿®å¤1ï¼šå…ˆè¿›è¡Œé™ç»´ï¼Œå†èšç±»
            if self.reduced_vectors is None:
                st.warning("âš ï¸ å»ºè®®å…ˆè¿›è¡Œé™ç»´ä»¥æé«˜DBSCANæ•ˆæžœï¼Œæ­£åœ¨ä½¿ç”¨åŽŸå§‹å‘é‡...")
                vectors_to_cluster = self.vectors
            else:
                vectors_to_cluster = self.reduced_vectors
                st.info(" ä½¿ç”¨é™ç»´åŽçš„å‘é‡è¿›è¡ŒDBSCANèšç±»")
            
            #  å…³é”®ä¿®å¤2ï¼šæ ¹æ®æ•°æ®ç»´åº¦é€‰æ‹©åˆé€‚çš„è·ç¦»åº¦é‡
            if vectors_to_cluster.shape[1] > 50:
                # é«˜ç»´æ•°æ®ä½¿ç”¨ä½™å¼¦è·ç¦»
                metric = 'cosine'
                # è‡ªåŠ¨è°ƒæ•´epsï¼ˆå¦‚æžœç”¨æˆ·ä½¿ç”¨é»˜è®¤å€¼ï¼‰
                if eps == 0.5:
                    eps = 0.3  # æ›´åˆç†çš„é»˜è®¤å€¼
                    st.info(f" é«˜ç»´æ•°æ®è‡ªåŠ¨è°ƒæ•´ eps={eps}")
            else:
                # ä½Žç»´æ•°æ®å¯ä»¥ä½¿ç”¨æ¬§æ°è·ç¦»
                metric = 'euclidean'
                st.info(f" ä½Žç»´æ•°æ®ä½¿ç”¨æ¬§æ°è·ç¦»")
            
            dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric=metric)
            self.cluster_labels = dbscan.fit_predict(vectors_to_cluster)
            
            #  å…³é”®ä¿®å¤3ï¼šè¯¦ç»†çš„èšç±»è¯Šæ–­ä¿¡æ¯
            unique_labels = set(self.cluster_labels)
            n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
            n_noise = list(self.cluster_labels).count(-1)
            
            # æ˜¾ç¤ºæ¯ä¸ªèšç±»çš„å¤§å°
            cluster_sizes = {}
            for label in unique_labels:
                if label != -1:
                    cluster_sizes[f"ç°‡ {label}"] = int((self.cluster_labels == label).sum())
            
            st.info(f"**âœ… DBSCANèšç±»å®Œæˆ**")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("èšç±»æ•°é‡", n_clusters)
            with col2:
                st.metric("å™ªå£°ç‚¹", f"{n_noise} ({n_noise/len(self.cluster_labels)*100:.1f}%)")
            with col3:
                st.metric("æœ‰æ•ˆç‚¹", len(self.cluster_labels) - n_noise)
            
            st.write(f"**å‚æ•°è®¾ç½®**: eps={eps}, min_samples={min_samples}, metric={metric}")
            
            if n_clusters > 0:
                st.write("**å„èšç±»å¤§å°**:", cluster_sizes)
            
            #  å…³é”®ä¿®å¤4ï¼šç»™å‡ºå‚æ•°è°ƒæ•´å»ºè®®
            if n_clusters == 0:
                st.warning("âš ï¸ **æœªå‘çŽ°ä»»ä½•èšç±»**ï¼Œå»ºè®®:")
                st.markdown(f"""
                -  å¢žå¤§ `eps` å€¼ï¼ˆå½“å‰: {eps}ï¼Œå»ºè®®å°è¯•: {eps*1.5:.2f}ï¼‰
                -  å‡å° `min_samples` å€¼ï¼ˆå½“å‰: {min_samples}ï¼Œå»ºè®®å°è¯•: {max(2, min_samples-2)}ï¼‰
                -  æˆ–å…ˆä½¿ç”¨ UMAP é™ç»´è‡³ 2-3 ç»´
                """)
            elif n_clusters == 1:
                st.warning("âš ï¸ **åªå‘çŽ°1ä¸ªèšç±»**ï¼Œå»ºè®®:")
                st.markdown(f"""
                -  å‡å° `eps` å€¼ä»¥åˆ†ç¦»æ›´å¤šèšç±»ï¼ˆå»ºè®®å°è¯•: {eps*0.7:.2f}ï¼‰
                -  æˆ–å¢žå¤§ `min_samples` ä»¥æé«˜å¯†åº¦è¦æ±‚ï¼ˆå»ºè®®å°è¯•: {min_samples+3}ï¼‰
                """)
            elif n_noise / len(self.cluster_labels) > 0.5:
                st.warning("âš ï¸ **å™ªå£°ç‚¹è¿‡å¤š**ï¼ˆ>{n_noise/len(self.cluster_labels)*100:.0f}%ï¼‰ï¼Œå»ºè®®:")
                st.markdown(f"""
                -  å¢žå¤§ `eps` å€¼ï¼ˆå»ºè®®å°è¯•: {eps*1.3:.2f}ï¼‰
                -  æˆ–å‡å° `min_samples` å€¼ï¼ˆå»ºè®®å°è¯•: {max(2, min_samples-2)}ï¼‰
                """)
            else:
                st.success("âœ… èšç±»ç»“æžœè‰¯å¥½ï¼")
            
            return self.cluster_labels
            
        except Exception as e:
            st.error(f"âŒ DBSCANèšç±»å¤±è´¥: {e}")
            import traceback
            st.code(traceback.format_exc())
            return np.array([])
    
    def find_optimal_dbscan_params(self, eps_range: List[float] = None, 
                                   min_samples_range: List[int] = None) -> Dict:
        """
        è‡ªåŠ¨æœç´¢æœ€ä¼˜çš„DBSCANå‚æ•°
        """
        if self.vectors is None:
            st.error("âŒ è¯·å…ˆåŠ è½½æ•°æ®")
            return {}
        
        # ä½¿ç”¨é™ç»´åŽçš„å‘é‡ï¼ˆå¦‚æžœå¯ç”¨ï¼‰
        vectors = self.reduced_vectors if self.reduced_vectors is not None else self.vectors
        
        st.info(f" å¼€å§‹æœç´¢æœ€ä¼˜DBSCANå‚æ•°ï¼ˆæ•°æ®ç»´åº¦: {vectors.shape[1]}ï¼‰")
        
        # é»˜è®¤æœç´¢èŒƒå›´
        if eps_range is None:
            if vectors.shape[1] > 50:  # é«˜ç»´
                eps_range = [0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5]
                metric = 'cosine'
            else:  # ä½Žç»´
                eps_range = np.linspace(0.3, 2.0, 7).tolist()
                metric = 'euclidean'
        else:
            metric = 'cosine' if vectors.shape[1] > 50 else 'euclidean'
        
        if min_samples_range is None:
            min_samples_range = [3, 5, 7, 10, 15]
        
        results = []
        best_score = -1
        best_params = {}
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        total_iterations = len(eps_range) * len(min_samples_range)
        iteration = 0
        
        for eps in eps_range:
            for min_samples in min_samples_range:
                status_text.text(f"æµ‹è¯•å‚æ•°: eps={eps:.2f}, min_samples={min_samples}")
                
                dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric=metric)
                labels = dbscan.fit_predict(vectors)
                
                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                n_noise = list(labels).count(-1)
                noise_ratio = n_noise / len(labels)
                
                # è¯„åˆ†ï¼šä¼˜å…ˆè€ƒè™‘æœ‰å¤šä¸ªèšç±»ä¸”å™ªå£°é€‚ä¸­çš„ç»“æžœ
                if n_clusters > 1 and noise_ratio < 0.5:
                    try:
                        score = silhouette_score(vectors, labels)
                        # ç»¼åˆè¯„åˆ†ï¼šè½®å»“ç³»æ•° - å™ªå£°æƒ©ç½š
                        score = score * (1 - noise_ratio * 0.5)
                    except:
                        score = 0
                else:
                    score = -1
                
                results.append({
                    'eps': eps,
                    'min_samples': min_samples,
                    'n_clusters': n_clusters,
                    'noise_ratio': f"{noise_ratio*100:.1f}%",
                    'noise_count': n_noise,
                    'score': score
                })
                
                if score > best_score:
                    best_score = score
                    best_params = {'eps': eps, 'min_samples': min_samples, 
                                  'n_clusters': n_clusters, 'score': score}
                
                iteration += 1
                progress_bar.progress(iteration / total_iterations)
        
        progress_bar.empty()
        status_text.empty()
        
        # æ˜¾ç¤ºç»“æžœ
        results_df = pd.DataFrame(results).sort_values('score', ascending=False)
        
        st.write("###  å‚æ•°æœç´¢ç»“æžœï¼ˆæŒ‰å¾—åˆ†æŽ’åºï¼Œå‰10åï¼‰")
        st.dataframe(
            results_df.head(10).style.format({
                'eps': '{:.3f}',
                'score': '{:.3f}'
            }),
            use_container_width=True
        )
        
        if best_score > -1:
            st.success(f"""
            ### âœ… æŽ¨èå‚æ•°
            - **eps**: {best_params['eps']}
            - **min_samples**: {best_params['min_samples']}
            - **é¢„æœŸèšç±»æ•°**: {best_params['n_clusters']}
            - **å¾—åˆ†**: {best_params['score']:.3f}
            """)
        else:
            st.warning("""
            ### âš ï¸ æœªæ‰¾åˆ°ç†æƒ³å‚æ•°
            å»ºè®®:
            - å…ˆè¿›è¡Œé™ç»´ (UMAPé™è‡³2-3ç»´)
            - æˆ–ä½¿ç”¨ K-means èšç±»
            - æˆ–è°ƒæ•´æœç´¢èŒƒå›´
            """)
        
        return best_params
    
    def reduce_dimensions(self, n_components: int = 2, random_state: int = 42) -> np.ndarray:
        """
        ä½¿ç”¨UMAPè¿›è¡Œé™ç»´
        
        ðŸ”§ ä¿®å¤è¯´æ˜Žï¼š
        1. ç§»é™¤äº† n_jobs å‚æ•°ï¼ˆä¸Ž random_state å†²çªï¼‰
        2. æ·»åŠ äº† low_memory å‚æ•°ä»¥é¿å… TBB è­¦å‘Š
        """
        if self.vectors is None:
            st.error("âŒ è¯·å…ˆåŠ è½½æ•°æ®")
            return np.array([])
        
        try:
            st.info(f" æ­£åœ¨è¿›è¡ŒUMAPé™ç»´...")
            
            # ðŸ”§ ä¿®å¤ï¼šç§»é™¤ n_jobsï¼Œä¿ç•™ random_state ä»¥ç¡®ä¿ç»“æžœå¯é‡å¤
            # low_memory=True å¯ä»¥é¿å…æŸäº› TBB ç›¸å…³è­¦å‘Š
            umap_reducer = UMAP(
                n_components=n_components, 
                random_state=random_state,  # ä¿ç•™ä»¥ç¡®ä¿å¯é‡å¤æ€§
                metric='cosine', 
                n_neighbors=15, 
                min_dist=0.1,
                low_memory=True  # å‡å°‘å†…å­˜ä½¿ç”¨ï¼Œé¿å… TBB è­¦å‘Š
                # æ³¨æ„ï¼šä¸è®¾ç½® n_jobs å‚æ•°
            )
            
            self.reduced_vectors = umap_reducer.fit_transform(self.vectors)
            
            st.success(f"âœ… UMAPé™ç»´å®Œæˆ: {self.vectors.shape[1]} ç»´ â†’ {n_components} ç»´")
            return self.reduced_vectors
            
        except Exception as e:
            st.error(f"âŒ UMAPé™ç»´å¤±è´¥: {e}")
            import traceback
            st.code(traceback.format_exc())
            return np.array([])
    
    def create_cluster_visualization(self, use_3d: bool = False) -> go.Figure:
        """
        åˆ›å»ºèšç±»å¯è§†åŒ–å›¾
        
        Args:
            use_3d: æ˜¯å¦åˆ›å»º3Då¯è§†åŒ–ï¼ˆéœ€è¦é™ç»´åˆ°3ç»´ï¼‰
        """
        if self.reduced_vectors is None or self.cluster_labels is None:
            st.error("âŒ è¯·å…ˆè¿›è¡Œé™ç»´å’Œèšç±»")
            return go.Figure()
        
        try:
            # åˆ›å»ºDataFrame
            df = pd.DataFrame({
                'x': self.reduced_vectors[:, 0],
                'y': self.reduced_vectors[:, 1],
                'cluster': self.cluster_labels.astype(str),
                'text': [text[:100] + '...' if len(text) > 100 else text for text in self.texts]
            })
            
            # æ ‡è®°å™ªå£°ç‚¹
            df['cluster'] = df['cluster'].apply(lambda x: 'å™ªå£°ç‚¹' if x == '-1' else f'ç°‡ {x}')
            
            # åˆ›å»ºæ•£ç‚¹å›¾
            if use_3d and self.reduced_vectors.shape[1] >= 3:
                df['z'] = self.reduced_vectors[:, 2]
                fig = px.scatter_3d(
                    df, 
                    x='x', 
                    y='y',
                    z='z',
                    color='cluster',
                    hover_data=['text'],
                    title='æ–‡æœ¬èšç±»å¯è§†åŒ– (3D)',
                    labels={'x': 'UMAP-1', 'y': 'UMAP-2', 'z': 'UMAP-3'}
                )
            else:
                fig = px.scatter(
                    df, 
                    x='x', 
                    y='y', 
                    color='cluster',
                    hover_data=['text'],
                    title='æ–‡æœ¬èšç±»å¯è§†åŒ– (2D)',
                    labels={'x': 'UMAPç»´åº¦1', 'y': 'UMAPç»´åº¦2'}
                )
            
            fig.update_layout(
                width=900,
                height=700,
                showlegend=True,
                legend=dict(
                    yanchor="top",
                    y=0.99,
                    xanchor="left",
                    x=0.01
                )
            )
            
            fig.update_traces(marker=dict(size=8, opacity=0.7))
            
            return fig
            
        except Exception as e:
            st.error(f"âŒ åˆ›å»ºå¯è§†åŒ–å¤±è´¥: {e}")
            import traceback
            st.code(traceback.format_exc())
            return go.Figure()
    
    def get_cluster_summary(self) -> Dict[str, Any]:
        """
        èŽ·å–èšç±»æ‘˜è¦ä¿¡æ¯
        """
        if self.cluster_labels is None:
            return {}
        
        try:
            unique_labels = np.unique(self.cluster_labels)
            cluster_summary = {}
            
            for label in unique_labels:
                mask = self.cluster_labels == label
                cluster_texts = [self.texts[i] for i in range(len(self.texts)) if mask[i]]
                
                label_name = "å™ªå£°ç‚¹" if label == -1 else f"ç°‡ {label}"
                
                cluster_summary[label_name] = {
                    'size': int(np.sum(mask)),
                    'percentage': float(np.sum(mask) / len(self.cluster_labels) * 100),
                    'sample_texts': cluster_texts[:5]  # æ˜¾ç¤ºå‰5ä¸ªæ ·æœ¬
                }
            
            return cluster_summary
            
        except Exception as e:
            st.error(f"âŒ èŽ·å–èšç±»æ‘˜è¦å¤±è´¥: {e}")
            return {}
    
    def find_optimal_k(self, max_k: int = 20) -> Tuple[List[int], List[float]]:
        """
        ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™æ‰¾åˆ°æœ€ä¼˜çš„Kå€¼
        """
        if self.vectors is None:
            st.error("âŒ è¯·å…ˆåŠ è½½æ•°æ®")
            return [], []
        
        try:
            k_range = range(2, min(max_k + 1, len(self.vectors)))
            inertias = []
            silhouette_scores = []
            
            st.info(f" æ­£åœ¨å¯»æ‰¾æœ€ä¼˜Kå€¼ (æµ‹è¯•èŒƒå›´: 2-{max(k_range)})...")
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for i, k in enumerate(k_range):
                status_text.text(f"æµ‹è¯• K={k}...")
                kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                labels = kmeans.fit_predict(self.vectors)
                
                inertias.append(kmeans.inertia_)
                silhouette_scores.append(silhouette_score(self.vectors, labels))
                
                progress_bar.progress((i + 1) / len(k_range))
            
            progress_bar.empty()
            status_text.empty()
            
            # æ‰¾åˆ°æœ€ä¼˜Kï¼ˆè½®å»“ç³»æ•°æœ€é«˜ï¼‰
            optimal_k = list(k_range)[np.argmax(silhouette_scores)]
            st.success(f"âœ… æŽ¨èKå€¼: {optimal_k} (è½®å»“ç³»æ•°: {max(silhouette_scores):.3f})")
            
            return list(k_range), silhouette_scores
            
        except Exception as e:
            st.error(f"âŒ å¯»æ‰¾æœ€ä¼˜Kå€¼å¤±è´¥: {e}")
            return [], []
    
    def export_cluster_results(self) -> pd.DataFrame:
        """
        å¯¼å‡ºèšç±»ç»“æžœ
        """
        if self.cluster_labels is None:
            st.error("âŒ è¯·å…ˆè¿›è¡Œèšç±»")
            return pd.DataFrame()
        
        try:
            results_df = pd.DataFrame({
                'text': self.texts,
                'cluster': self.cluster_labels,
                'cluster_name': ['å™ªå£°ç‚¹' if c == -1 else f'ç°‡ {c}' for c in self.cluster_labels],
                'metadata': [str(meta) for meta in self.metadata]
            })
            
            if self.reduced_vectors is not None:
                results_df['umap_x'] = self.reduced_vectors[:, 0]
                results_df['umap_y'] = self.reduced_vectors[:, 1]
                if self.reduced_vectors.shape[1] >= 3:
                    results_df['umap_z'] = self.reduced_vectors[:, 2]
            
            st.success(f"âœ… æˆåŠŸå¯¼å‡º {len(results_df)} æ¡èšç±»ç»“æžœ")
            return results_df
            
        except Exception as e:
            st.error(f"âŒ å¯¼å‡ºç»“æžœå¤±è´¥: {e}")
            return pd.DataFrame()
    
    def compare_clustering_methods(self, k_values: List[int] = [5, 8, 10]) -> pd.DataFrame:
        """
        æ¯”è¾ƒä¸åŒèšç±»æ–¹æ³•çš„æ•ˆæžœ
        """
        if self.vectors is None:
            st.error("âŒ è¯·å…ˆåŠ è½½æ•°æ®")
            return pd.DataFrame()
        
        results = []
        st.info(" æ­£åœ¨æ¯”è¾ƒä¸åŒèšç±»æ–¹æ³•...")
        
        # æµ‹è¯•K-means
        for k in k_values:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            labels = kmeans.fit_predict(self.vectors)
            score = silhouette_score(self.vectors, labels)
            results.append({
                'method': f'K-means (k={k})',
                'n_clusters': k,
                'silhouette_score': score
            })
        
        # æµ‹è¯•DBSCANï¼ˆå¦‚æžœå·²é™ç»´ï¼‰
        if self.reduced_vectors is not None:
            for eps in [0.2, 0.3, 0.5]:
                dbscan = DBSCAN(eps=eps, min_samples=5, metric='euclidean')
                labels = dbscan.fit_predict(self.reduced_vectors)
                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                if n_clusters > 1:
                    try:
                        score = silhouette_score(self.reduced_vectors, labels)
                    except:
                        score = -1
                else:
                    score = -1
                results.append({
                    'method': f'DBSCAN (eps={eps})',
                    'n_clusters': n_clusters,
                    'silhouette_score': score
                })
        
        results_df = pd.DataFrame(results).sort_values('silhouette_score', ascending=False)
        st.dataframe(results_df, use_container_width=True)
        
        return results_df
