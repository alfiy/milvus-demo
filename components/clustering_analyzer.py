import numpy as np
import pandas as pd
from sklearn.cluster import KMeans, DBSCAN
import plotly.express as px
import plotly.graph_objects as go
from umap import UMAP
import streamlit as st
from typing import List, Dict, Any, Tuple
import warnings
import hdbscan
from itertools import product
from sklearn.metrics import davies_bouldin_score, silhouette_score, davies_bouldin_score



# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings('ignore', message='.*n_jobs.*overridden.*')
warnings.filterwarnings('ignore', message='.*TBB threading layer.*')
warnings.filterwarnings('ignore', category=FutureWarning, module='sklearn')

class ClusteringAnalyzer:
    def __init__(self):
        """
        åˆå§‹åŒ–èšç±»åˆ†æå™¨
        """
        self.vectors = None
        self.texts = None
        self.metadata = None
        self.cluster_labels = None
        self.reduced_vectors = None
    
    def load_data(self, vectors: np.ndarray, texts: List[str], metadata: List[Dict]):
        """
        åŠ è½½æ•°æ®
        """
        self.vectors = vectors
        self.texts = texts
        self.metadata = metadata
    
    def perform_kmeans_clustering(self, n_clusters: int = 8, random_state: int = 42) -> np.ndarray:
        """
        æ‰§è¡ŒK-meansèšç±»
        """
        if self.vectors is None:
            st.error("è¯·å…ˆåŠ è½½æ•°æ®")
            return np.array([])
        
        try:
            kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)
            self.cluster_labels = kmeans.fit_predict(self.vectors)
            
            # è®¡ç®—è½®å»“ç³»æ•°
            if len(set(self.cluster_labels)) > 1:
                silhouette_avg = silhouette_score(self.vectors, self.cluster_labels)
                st.info(f"âœ… K-meansèšç±»å®Œæˆï¼Œè½®å»“ç³»æ•°: {silhouette_avg:.3f}")
            
            return self.cluster_labels
            
        except Exception as e:
            st.error(f"âŒ K-meansèšç±»å¤±è´¥: {e}")
            return np.array([])
    
    def perform_dbscan_clustering(self, eps: float = 0.3, min_samples: int = 5) -> np.ndarray:
        """
        æ‰§è¡ŒDBSCANèšç±»
        
        Args:
            eps: é‚»åŸŸåŠå¾„
                - cosineè·ç¦»ï¼šæ¨èèŒƒå›´ 0.1-0.5ï¼ˆå€¼è¶Šå°è¶Šä¸¥æ ¼ï¼‰
                - euclideanè·ç¦»ï¼šéœ€è¦æ ¹æ®æ•°æ®å°ºåº¦è°ƒæ•´
            min_samples: æ ¸å¿ƒç‚¹çš„æœ€å°é‚»å±…æ•°ï¼Œæ¨è 5-10
        """
        if self.vectors is None:
            st.error("âŒ è¯·å…ˆåŠ è½½æ•°æ®")
            return np.array([])
        
        try:
            #  å…³é”®ä¿®å¤1ï¼šå…ˆè¿›è¡Œé™ç»´ï¼Œå†èšç±»
            if self.reduced_vectors is None:
                st.warning("âš ï¸ å»ºè®®å…ˆè¿›è¡Œé™ç»´ä»¥æé«˜DBSCANæ•ˆæœï¼Œæ­£åœ¨ä½¿ç”¨åŸå§‹å‘é‡...")
                vectors_to_cluster = self.vectors
            else:
                vectors_to_cluster = self.reduced_vectors
                st.info(" ä½¿ç”¨é™ç»´åçš„å‘é‡è¿›è¡ŒDBSCANèšç±»")
            
            #  å…³é”®ä¿®å¤2ï¼šæ ¹æ®æ•°æ®ç»´åº¦é€‰æ‹©åˆé€‚çš„è·ç¦»åº¦é‡
            if vectors_to_cluster.shape[1] > 50:
                # é«˜ç»´æ•°æ®ä½¿ç”¨ä½™å¼¦è·ç¦»
                metric = 'cosine'
                # è‡ªåŠ¨è°ƒæ•´epsï¼ˆå¦‚æœç”¨æˆ·ä½¿ç”¨é»˜è®¤å€¼ï¼‰
                if eps == 0.5:
                    eps = 0.3  # æ›´åˆç†çš„é»˜è®¤å€¼
                    st.info(f" é«˜ç»´æ•°æ®è‡ªåŠ¨è°ƒæ•´ eps={eps}")
            else:
                # ä½ç»´æ•°æ®å¯ä»¥ä½¿ç”¨æ¬§æ°è·ç¦»
                metric = 'euclidean'
                st.info(f" ä½ç»´æ•°æ®ä½¿ç”¨æ¬§æ°è·ç¦»")
            
            dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric=metric)
            self.cluster_labels = dbscan.fit_predict(vectors_to_cluster)
            
            #  å…³é”®ä¿®å¤3ï¼šè¯¦ç»†çš„èšç±»è¯Šæ–­ä¿¡æ¯
            unique_labels = set(self.cluster_labels)
            n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
            n_noise = list(self.cluster_labels).count(-1)
            
            # æ˜¾ç¤ºæ¯ä¸ªèšç±»çš„å¤§å°
            cluster_sizes = {}
            for label in unique_labels:
                if label != -1:
                    cluster_sizes[f"ç°‡ {label}"] = int((self.cluster_labels == label).sum())
            
            st.info(f"**âœ… DBSCANèšç±»å®Œæˆ**")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("èšç±»æ•°é‡", n_clusters)
            with col2:
                st.metric("å™ªå£°ç‚¹", f"{n_noise} ({n_noise/len(self.cluster_labels)*100:.1f}%)")
            with col3:
                st.metric("æœ‰æ•ˆç‚¹", len(self.cluster_labels) - n_noise)
            
            st.write(f"**å‚æ•°è®¾ç½®**: eps={eps}, min_samples={min_samples}, metric={metric}")
            
            if n_clusters > 0:
                st.write("**å„èšç±»å¤§å°**:", cluster_sizes)
            
            #  å…³é”®ä¿®å¤4ï¼šç»™å‡ºå‚æ•°è°ƒæ•´å»ºè®®
            if n_clusters == 0:
                st.warning("âš ï¸ **æœªå‘ç°ä»»ä½•èšç±»**ï¼Œå»ºè®®:")
                st.markdown(f"""
                -  å¢å¤§ `eps` å€¼ï¼ˆå½“å‰: {eps}ï¼Œå»ºè®®å°è¯•: {eps*1.5:.2f}ï¼‰
                -  å‡å° `min_samples` å€¼ï¼ˆå½“å‰: {min_samples}ï¼Œå»ºè®®å°è¯•: {max(2, min_samples-2)}ï¼‰
                -  æˆ–å…ˆä½¿ç”¨ UMAP é™ç»´è‡³ 2-3 ç»´
                """)
            elif n_clusters == 1:
                st.warning("âš ï¸ **åªå‘ç°1ä¸ªèšç±»**ï¼Œå»ºè®®:")
                st.markdown(f"""
                -  å‡å° `eps` å€¼ä»¥åˆ†ç¦»æ›´å¤šèšç±»ï¼ˆå»ºè®®å°è¯•: {eps*0.7:.2f}ï¼‰
                -  æˆ–å¢å¤§ `min_samples` ä»¥æé«˜å¯†åº¦è¦æ±‚ï¼ˆå»ºè®®å°è¯•: {min_samples+3}ï¼‰
                """)
            elif n_noise / len(self.cluster_labels) > 0.5:
                st.warning("âš ï¸ **å™ªå£°ç‚¹è¿‡å¤š**ï¼ˆ>{n_noise/len(self.cluster_labels)*100:.0f}%ï¼‰ï¼Œå»ºè®®:")
                st.markdown(f"""
                -  å¢å¤§ `eps` å€¼ï¼ˆå»ºè®®å°è¯•: {eps*1.3:.2f}ï¼‰
                -  æˆ–å‡å° `min_samples` å€¼ï¼ˆå»ºè®®å°è¯•: {max(2, min_samples-2)}ï¼‰
                """)
            else:
                st.success("âœ… èšç±»ç»“æœè‰¯å¥½ï¼")
            
            return self.cluster_labels
            
        except Exception as e:
            st.error(f"âŒ DBSCANèšç±»å¤±è´¥: {e}")
            import traceback
            st.code(traceback.format_exc())
            return np.array([])
    
    def find_optimal_dbscan_params(self, eps_range: List[float] = None, 
                                   min_samples_range: List[int] = None) -> Dict:
        """
        è‡ªåŠ¨æœç´¢æœ€ä¼˜çš„DBSCANå‚æ•°
        """
        if self.vectors is None:
            st.error("âŒ è¯·å…ˆåŠ è½½æ•°æ®")
            return {}
        
        # ä½¿ç”¨é™ç»´åçš„å‘é‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        vectors = self.reduced_vectors if self.reduced_vectors is not None else self.vectors
        
        st.info(f" å¼€å§‹æœç´¢æœ€ä¼˜DBSCANå‚æ•°ï¼ˆæ•°æ®ç»´åº¦: {vectors.shape[1]}ï¼‰")
        
        # é»˜è®¤æœç´¢èŒƒå›´
        if eps_range is None:
            if vectors.shape[1] > 50:  # é«˜ç»´
                eps_range = [0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5]
                metric = 'cosine'
            else:  # ä½ç»´
                eps_range = np.linspace(0.3, 2.0, 7).tolist()
                metric = 'euclidean'
        else:
            metric = 'cosine' if vectors.shape[1] > 50 else 'euclidean'
        
        if min_samples_range is None:
            min_samples_range = [3, 5, 7, 10, 15]
        
        results = []
        best_score = -1
        best_params = {}
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        total_iterations = len(eps_range) * len(min_samples_range)
        iteration = 0
        
        for eps in eps_range:
            for min_samples in min_samples_range:
                status_text.text(f"æµ‹è¯•å‚æ•°: eps={eps:.2f}, min_samples={min_samples}")
                
                dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric=metric)
                labels = dbscan.fit_predict(vectors)
                
                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                n_noise = list(labels).count(-1)
                noise_ratio = n_noise / len(labels)
                
                # è¯„åˆ†ï¼šä¼˜å…ˆè€ƒè™‘æœ‰å¤šä¸ªèšç±»ä¸”å™ªå£°é€‚ä¸­çš„ç»“æœ
                if n_clusters > 1 and noise_ratio < 0.5:
                    try:
                        score = silhouette_score(vectors, labels)
                        # ç»¼åˆè¯„åˆ†ï¼šè½®å»“ç³»æ•° - å™ªå£°æƒ©ç½š
                        score = score * (1 - noise_ratio * 0.5)
                    except:
                        score = 0
                else:
                    score = -1
                
                results.append({
                    'eps': eps,
                    'min_samples': min_samples,
                    'n_clusters': n_clusters,
                    'noise_ratio': f"{noise_ratio*100:.1f}%",
                    'noise_count': n_noise,
                    'score': score
                })
                
                if score > best_score:
                    best_score = score
                    best_params = {'eps': eps, 'min_samples': min_samples, 
                                  'n_clusters': n_clusters, 'score': score}
                
                iteration += 1
                progress_bar.progress(iteration / total_iterations)
        
        progress_bar.empty()
        status_text.empty()
        
        # æ˜¾ç¤ºç»“æœ
        results_df = pd.DataFrame(results).sort_values('score', ascending=False)
        
        st.write("###  å‚æ•°æœç´¢ç»“æœï¼ˆæŒ‰å¾—åˆ†æ’åºï¼Œå‰10åï¼‰")
        st.dataframe(
            results_df.head(10).style.format({
                'eps': '{:.3f}',
                'score': '{:.3f}'
            }),
            use_container_width=True
        )
        
        if best_score > -1:
            st.success(f"""
            ### âœ… æ¨èå‚æ•°
            - **eps**: {best_params['eps']}
            - **min_samples**: {best_params['min_samples']}
            - **é¢„æœŸèšç±»æ•°**: {best_params['n_clusters']}
            - **å¾—åˆ†**: {best_params['score']:.3f}
            """)
        else:
            st.warning("""
            ### âš ï¸ æœªæ‰¾åˆ°ç†æƒ³å‚æ•°
            å»ºè®®:
            - å…ˆè¿›è¡Œé™ç»´ (UMAPé™è‡³2-3ç»´)
            - æˆ–ä½¿ç”¨ K-means èšç±»
            - æˆ–è°ƒæ•´æœç´¢èŒƒå›´
            """)
        
        return best_params
    
    # hdbscanèšç±»ç®—æ³•
    def perform_hdbscan_clustering(
        self, 
        min_cluster_size: int = 5, 
        min_samples: int = None,
        cluster_selection_epsilon: float = 0.0
    ) -> np.ndarray:
        """
        æ‰§è¡Œ HDBSCAN èšç±»
        Args:
            min_cluster_size: æ¯ä¸ªèšç±»çš„æœ€å°æ ·æœ¬æ•°ï¼ˆæ¨è 5-15ï¼‰
            min_samples: æ§åˆ¶å™ªå£°æ•æ„Ÿåº¦ï¼ˆé»˜è®¤ç­‰äº min_cluster_sizeï¼‰
            cluster_selection_epsilon: æ§åˆ¶èšç±»è¾¹ç•Œçš„å®½æ¾ç¨‹åº¦ï¼ˆé»˜è®¤ 0ï¼‰
        """
        

        if self.vectors is None:
            st.error("âŒ è¯·å…ˆåŠ è½½æ•°æ®")
            return np.array([])

        try:
            # é€‰æ‹©èšç±»è¾“å…¥å‘é‡
            if self.reduced_vectors is not None:
                vectors_to_cluster = self.reduced_vectors
                st.info(" ä½¿ç”¨é™ç»´åçš„å‘é‡è¿›è¡Œ HDBSCAN èšç±»")
            else:
                st.warning("âš ï¸ æœªè¿›è¡Œé™ç»´ï¼Œæ­£åœ¨ä½¿ç”¨åŸå§‹å‘é‡è¿›è¡Œ HDBSCAN èšç±»ï¼ˆå»ºè®®å…ˆ UMAP é™ç»´ï¼‰")
                vectors_to_cluster = self.vectors

            # è‡ªåŠ¨é€‰æ‹©åº¦é‡æ–¹å¼
            if vectors_to_cluster.shape[1] > 50:
                metric = 'cosine'
                st.info(" é«˜ç»´æ•°æ®ä½¿ç”¨ä½™å¼¦è·ç¦» (cosine)")
            else:
                metric = 'euclidean'
                st.info(" ä½ç»´æ•°æ®ä½¿ç”¨æ¬§æ°è·ç¦» (euclidean)")

            # åˆ›å»ºå¹¶æ‹Ÿåˆ HDBSCAN æ¨¡å‹
            clusterer = hdbscan.HDBSCAN(
                min_cluster_size=min_cluster_size,
                min_samples=min_samples,
                metric=metric,
                cluster_selection_epsilon=cluster_selection_epsilon,
                cluster_selection_method='eom'
            )

            self.cluster_labels = clusterer.fit_predict(vectors_to_cluster)
            unique_labels = set(self.cluster_labels)
            n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
            n_noise = list(self.cluster_labels).count(-1)

            st.info("**âœ… HDBSCAN èšç±»å®Œæˆ**")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("èšç±»æ•°é‡", n_clusters)
            with col2:
                st.metric("å™ªå£°ç‚¹", f"{n_noise} ({n_noise/len(self.cluster_labels)*100:.1f}%)")
            with col3:
                st.metric("æœ‰æ•ˆç‚¹", len(self.cluster_labels) - n_noise)

            # å„èšç±»å¤§å°
            cluster_sizes = {
                f"ç°‡ {label}": int((self.cluster_labels == label).sum())
                for label in unique_labels if label != -1
            }
            if cluster_sizes:
                st.write("**å„èšç±»å¤§å°:**", cluster_sizes)

            # å°è¯•è®¡ç®—è½®å»“ç³»æ•°ï¼ˆéœ€å¤šä¸ªèšç±»ä¸”å™ªå£°ç‚¹ < 90%ï¼‰
            silhouette_avg = None
            if n_clusters > 1 and n_noise / len(self.cluster_labels) < 0.9:
                from sklearn.metrics import silhouette_score
                silhouette_avg = silhouette_score(vectors_to_cluster, self.cluster_labels)
                st.success(f"å¹³å‡è½®å»“ç³»æ•°: {silhouette_avg:.3f}")

            # å‚æ•°å»ºè®®
            if n_clusters == 0:
                st.warning("âš ï¸ æœªå‘ç°ä»»ä½•èšç±»ã€‚å»ºè®®ï¼š")
                st.markdown(f"""
                - å‡å° `min_cluster_size`ï¼ˆå½“å‰ {min_cluster_size} â†’ å»ºè®® {max(2, min_cluster_size // 2)}ï¼‰
                - æˆ–å¢åŠ é™ç»´ç»´åº¦
                """)
            elif n_clusters == 1:
                st.warning("âš ï¸ åªå‘ç° 1 ä¸ªèšç±»ã€‚å»ºè®®ï¼š")
                st.markdown(f"""
                - å¢å¤§ `min_cluster_size` æˆ– `min_samples`
                - è°ƒæ•´ `cluster_selection_epsilon` (å½“å‰ {cluster_selection_epsilon} â†’ å»ºè®® {cluster_selection_epsilon + 0.05})
                """)
            elif n_noise / len(self.cluster_labels) > 0.5:
                st.warning(f"âš ï¸ å™ªå£°ç‚¹è¿‡å¤š ({n_noise/len(self.cluster_labels)*100:.1f}%)ã€‚å»ºè®®ï¼š")
                st.markdown(f"""
                - å‡å° `min_samples`ï¼ˆå½“å‰ {min_samples or min_cluster_size} â†’ å»ºè®® {max(2, (min_samples or min_cluster_size)//2)}ï¼‰
                - å¢åŠ  `cluster_selection_epsilon`
                """)

            else:
                st.success("âœ… èšç±»ç»“æœè‰¯å¥½ï¼")

            return self.cluster_labels

        except Exception as e:
            st.error(f"âŒ HDBSCAN èšç±»å¤±è´¥: {e}")
            import traceback
            st.code(traceback.format_exc())
            return np.array([])

    # è‡ªåŠ¨ä¼˜åŒ–hdbscanå‚æ•° 
    def find_optimal_hdbscan_params(
        self,
        min_cluster_size_range: Tuple[int, int, int] = (5, 50, 5),
        min_samples_range: Tuple[int, int, int] = (1, 10, 2),
        metrics: Tuple[str, ...] = ("euclidean", "manhattan"),
        cluster_selection_methods: Tuple[str, ...] = ("eom", "leaf"),
        scoring: str = "silhouette",
    ):
        """
        è‡ªåŠ¨æœç´¢æœ€ä½³ HDBSCAN èšç±»å‚æ•°ï¼ˆå¸¦ Streamlit å®æ—¶è¿›åº¦ä¸è¡¨æ ¼å±•ç¤ºï¼‰
        """

        if self.vectors is None:
            st.error("âŒ è¯·å…ˆåŠ è½½æˆ–ç”ŸæˆåµŒå…¥å‘é‡ï¼")
            return None, None, None, None

        st.subheader("ğŸ” HDBSCAN å‚æ•°æœç´¢ä¸­...")
        progress_bar = st.progress(0)
        status_text = st.empty()
        result_table = st.empty()

        embeddings = self.reduced_vectors if self.reduced_vectors is not None else self.vectors

        min_cluster_sizes = range(*min_cluster_size_range)
        min_samples = range(*min_samples_range)
        total = len(list(product(min_cluster_sizes, min_samples, metrics, cluster_selection_methods)))
        current = 0

        all_results = []
        best_score = -np.inf if scoring == "silhouette" else np.inf
        best_model, best_params = None, None

        for mcs, ms, metric, method in product(min_cluster_sizes, min_samples, metrics, cluster_selection_methods):
            current += 1
            progress = current / total
            status_text.text(f"æ­£åœ¨æœç´¢: min_cluster_size={mcs}, min_samples={ms}, metric={metric}, method={method} ({current}/{total})")
            progress_bar.progress(progress)

            try:
                model = hdbscan.HDBSCAN(
                    min_cluster_size=mcs,
                    min_samples=ms,
                    metric=metric,
                    cluster_selection_method=method,
                    prediction_data=True,
                    gen_min_span_tree=False
                ).fit(embeddings)

                labels = model.labels_
                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)

                # è·³è¿‡å…¨å™ªå£°æˆ–ä»…ä¸€ä¸ªç°‡çš„æƒ…å†µ
                if n_clusters < 2:
                    continue

                if scoring == "silhouette":
                    score = silhouette_score(embeddings, labels)
                    better = score > best_score
                else:
                    score = davies_bouldin_score(embeddings, labels)
                    better = score < best_score

                result = {
                    "min_cluster_size": mcs,
                    "min_samples": ms,
                    "metric": metric,
                    "method": method,
                    "n_clusters": n_clusters,
                    "score": round(score, 4),
                }
                all_results.append(result)

                if better:
                    best_score = score
                    best_model = model
                    best_params = {
                        "min_cluster_size": mcs,
                        "min_samples": ms,
                        "metric": metric,
                        "cluster_selection_method": method,
                        "n_clusters": n_clusters,
                    }

                    st.success(f"âœ¨ æ–°æœ€ä¼˜ç»„åˆ: {best_params} | score={score:.4f}")

                # æ¯æ¬¡æ›´æ–°è¡¨æ ¼
                df = pd.DataFrame(all_results).sort_values(
                    "score", ascending=(scoring != "silhouette"), ignore_index=True
                )
                result_table.dataframe(df)

            except Exception as e:
                st.warning(f"âš ï¸ å‚æ•°ç»„åˆå¤±è´¥: min_cluster_size={mcs}, min_samples={ms}, metric={metric} ({e})")
                continue

        progress_bar.progress(1.0)
        status_text.text("âœ… æœç´¢å®Œæˆï¼")

        if not all_results:
            st.error("âŒ æœªæ‰¾åˆ°åˆé€‚çš„èšç±»ç»“æœï¼Œè¯·è°ƒæ•´æœç´¢èŒƒå›´æˆ–å‚æ•°ã€‚")
            return None, None, None, None

        st.success(f"ğŸ æœ€ä½³å‚æ•°: {best_params}")
        st.metric("æœ€ä½³åˆ†æ•°", f"{best_score:.4f}")
        st.metric("æœ€ä½³èšç±»æ•°", best_params['n_clusters'])

        return best_model, best_params, best_score, all_results


    def reduce_dimensions(self, n_components: int = 2, random_state: int = 42) -> np.ndarray:
        """
        ä½¿ç”¨UMAPè¿›è¡Œé™ç»´
        
        """
        if self.vectors is None:
            st.error("âŒ è¯·å…ˆåŠ è½½æ•°æ®")
            return np.array([])
        
        try:
            st.info(f" æ­£åœ¨è¿›è¡ŒUMAPé™ç»´...")
            
            # low_memory=True å¯ä»¥é¿å…æŸäº› TBB ç›¸å…³è­¦å‘Š
            umap_reducer = UMAP(
                n_components=n_components, 
                random_state=random_state,  # ä¿ç•™ä»¥ç¡®ä¿å¯é‡å¤æ€§
                metric='cosine', 
                n_neighbors=15, 
                min_dist=0.1,
                low_memory=True  # å‡å°‘å†…å­˜ä½¿ç”¨ï¼Œé¿å… TBB è­¦å‘Š
            )
            
            self.reduced_vectors = umap_reducer.fit_transform(self.vectors)
            
            st.success(f"âœ… UMAPé™ç»´å®Œæˆ: {self.vectors.shape[1]} ç»´ â†’ {n_components} ç»´")
            return self.reduced_vectors
            
        except Exception as e:
            st.error(f"âŒ UMAPé™ç»´å¤±è´¥: {e}")
            import traceback
            st.code(traceback.format_exc())
            return np.array([])
    
    def create_cluster_visualization(self, use_3d: bool = False) -> go.Figure:
        """
        åˆ›å»ºèšç±»å¯è§†åŒ–å›¾
        
        Args:
            use_3d: æ˜¯å¦åˆ›å»º3Då¯è§†åŒ–ï¼ˆéœ€è¦é™ç»´åˆ°3ç»´ï¼‰
        """
        if self.reduced_vectors is None or self.cluster_labels is None:
            st.error("âŒ è¯·å…ˆè¿›è¡Œé™ç»´å’Œèšç±»")
            return go.Figure()
        
        try:
            # åˆ›å»ºDataFrame
            df = pd.DataFrame({
                'x': self.reduced_vectors[:, 0],
                'y': self.reduced_vectors[:, 1],
                'cluster': self.cluster_labels.astype(str),
                'text': [text[:100] + '...' if len(text) > 100 else text for text in self.texts]
            })
            
            # æ ‡è®°å™ªå£°ç‚¹
            df['cluster'] = df['cluster'].apply(lambda x: 'å™ªå£°ç‚¹' if x == '-1' else f'ç°‡ {x}')
            
            # åˆ›å»ºæ•£ç‚¹å›¾
            if use_3d and self.reduced_vectors.shape[1] >= 3:
                df['z'] = self.reduced_vectors[:, 2]
                fig = px.scatter_3d(
                    df, 
                    x='x', 
                    y='y',
                    z='z',
                    color='cluster',
                    hover_data=['text'],
                    title='æ–‡æœ¬èšç±»å¯è§†åŒ– (3D)',
                    labels={'x': 'UMAP-1', 'y': 'UMAP-2', 'z': 'UMAP-3'}
                )
            else:
                fig = px.scatter(
                    df, 
                    x='x', 
                    y='y', 
                    color='cluster',
                    hover_data=['text'],
                    title='æ–‡æœ¬èšç±»å¯è§†åŒ– (2D)',
                    labels={'x': 'UMAPç»´åº¦1', 'y': 'UMAPç»´åº¦2'}
                )
            
            fig.update_layout(
                width=900,
                height=700,
                showlegend=True,
                legend=dict(
                    yanchor="top",
                    y=0.99,
                    xanchor="left",
                    x=0.01
                )
            )
            
            fig.update_traces(marker=dict(size=8, opacity=0.7))
            
            return fig
            
        except Exception as e:
            st.error(f"âŒ åˆ›å»ºå¯è§†åŒ–å¤±è´¥: {e}")
            import traceback
            st.code(traceback.format_exc())
            return go.Figure()
    
    def get_cluster_summary(self) -> Dict[str, Any]:
        """
        è·å–èšç±»æ‘˜è¦ä¿¡æ¯
        """
        if self.cluster_labels is None:
            return {}
        
        try:
            unique_labels = np.unique(self.cluster_labels)
            cluster_summary = {}
            
            for label in unique_labels:
                mask = self.cluster_labels == label
                cluster_texts = [self.texts[i] for i in range(len(self.texts)) if mask[i]]
                
                label_name = "å™ªå£°ç‚¹" if label == -1 else f"ç°‡ {label}"
                
                cluster_summary[label_name] = {
                    'size': int(np.sum(mask)),
                    'percentage': float(np.sum(mask) / len(self.cluster_labels) * 100),
                    'sample_texts': cluster_texts[:5]  # æ˜¾ç¤ºå‰5ä¸ªæ ·æœ¬
                }
            
            return cluster_summary
            
        except Exception as e:
            st.error(f"âŒ è·å–èšç±»æ‘˜è¦å¤±è´¥: {e}")
            return {}
    
    def find_optimal_k(self, max_k: int = 20) -> Tuple[List[int], List[float]]:
        """
        ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™æ‰¾åˆ°æœ€ä¼˜çš„Kå€¼
        """
        if self.vectors is None:
            st.error("âŒ è¯·å…ˆåŠ è½½æ•°æ®")
            return [], []
        
        try:
            st.subheader("ğŸ” K-means æœ€ä¼˜Kå€¼æœç´¢ä¸­...")
            progress_bar = st.progress(0)
            status_text = st.empty()
            result_table = st.empty()
            
            k_range = range(2, min(max_k + 1, len(self.vectors)))
            total = len(k_range)
            current = 0
            
            all_results = []
            best_score = -np.inf
            best_k = None
            
            for k in k_range:
                current += 1
                progress = current / total
                status_text.text(f"æ­£åœ¨æœç´¢: K={k} ({current}/{total})")
                progress_bar.progress(progress)
                
                try:
                    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                    labels = kmeans.fit_predict(self.vectors)
                    
                    inertia = kmeans.inertia_
                    silhouette = silhouette_score(self.vectors, labels)
                    
                    result = {
                        "K": k,
                        "inertia": round(inertia, 2),
                        "silhouette_score": round(silhouette, 4),
                    }
                    all_results.append(result)
                    
                    if silhouette > best_score:
                        best_score = silhouette
                        best_k = k
                        st.success(f"âœ¨ æ–°æœ€ä¼˜Kå€¼: K={k} | silhouette={silhouette:.4f}")
                    
                    # æ¯æ¬¡æ›´æ–°è¡¨æ ¼
                    df = pd.DataFrame(all_results).sort_values(
                        "silhouette_score", ascending=False, ignore_index=True
                    )
                    result_table.dataframe(df)
                    
                except Exception as e:
                    st.warning(f"âš ï¸ K={k} æµ‹è¯•å¤±è´¥: ({e})")
                    continue
            
            progress_bar.progress(1.0)
            status_text.text("âœ… æœç´¢å®Œæˆ!")
            
            if not all_results:
                st.error("âŒ æœªæ‰¾åˆ°åˆé€‚çš„Kå€¼,è¯·æ£€æŸ¥æ•°æ®ã€‚")
                return [], []
            
            st.success(f"ğŸ¯ æœ€ä½³Kå€¼: {best_k}")
            st.metric("æœ€ä½³è½®å»“ç³»æ•°", f"{best_score:.4f}")
            
            # æå–æ•°æ®ç”¨äºè¿”å›
            k_values = [r["K"] for r in all_results]
            silhouette_scores = [r["silhouette_score"] for r in all_results]
            
            return k_values, silhouette_scores        
            
        except Exception as e:
            st.error(f"âŒ å¯»æ‰¾æœ€ä¼˜Kå€¼å¤±è´¥: {e}")
            return [], []
    
    def export_cluster_results(self) -> pd.DataFrame:
        """
        å¯¼å‡ºèšç±»ç»“æœ
        """
        if self.cluster_labels is None:
            st.error("âŒ è¯·å…ˆè¿›è¡Œèšç±»")
            return pd.DataFrame()
        
        try:
            results_df = pd.DataFrame({
                'text': self.texts,
                'cluster': self.cluster_labels,
                'cluster_name': ['å™ªå£°ç‚¹' if c == -1 else f'ç°‡ {c}' for c in self.cluster_labels],
                'metadata': [str(meta) for meta in self.metadata]
            })
            
            if self.reduced_vectors is not None:
                results_df['umap_x'] = self.reduced_vectors[:, 0]
                results_df['umap_y'] = self.reduced_vectors[:, 1]
                if self.reduced_vectors.shape[1] >= 3:
                    results_df['umap_z'] = self.reduced_vectors[:, 2]
            
            st.success(f"âœ… æˆåŠŸå¯¼å‡º {len(results_df)} æ¡èšç±»ç»“æœ")
            return results_df
            
        except Exception as e:
            st.error(f"âŒ å¯¼å‡ºç»“æœå¤±è´¥: {e}")
            return pd.DataFrame()
    
    def compare_clustering_methods(self, k_values: List[int] = [5, 8, 10]) -> pd.DataFrame:
        """
        æ¯”è¾ƒä¸åŒèšç±»æ–¹æ³•çš„æ•ˆæœ
        """
        if self.vectors is None:
            st.error("âŒ è¯·å…ˆåŠ è½½æ•°æ®")
            return pd.DataFrame()
        
        results = []
        st.info(" æ­£åœ¨æ¯”è¾ƒä¸åŒèšç±»æ–¹æ³•...")
        
        # æµ‹è¯•K-means
        for k in k_values:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            labels = kmeans.fit_predict(self.vectors)
            score = silhouette_score(self.vectors, labels)
            results.append({
                'method': f'K-means (k={k})',
                'n_clusters': k,
                'silhouette_score': score
            })
        
        # æµ‹è¯•DBSCANï¼ˆå¦‚æœå·²é™ç»´ï¼‰
        if self.reduced_vectors is not None:
            for eps in [0.2, 0.3, 0.5]:
                dbscan = DBSCAN(eps=eps, min_samples=5, metric='euclidean')
                labels = dbscan.fit_predict(self.reduced_vectors)
                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                if n_clusters > 1:
                    try:
                        score = silhouette_score(self.reduced_vectors, labels)
                    except:
                        score = -1
                else:
                    score = -1
                results.append({
                    'method': f'DBSCAN (eps={eps})',
                    'n_clusters': n_clusters,
                    'silhouette_score': score
                })
        
        results_df = pd.DataFrame(results).sort_values('silhouette_score', ascending=False)
        st.dataframe(results_df, use_container_width=True)
        
        return results_df
